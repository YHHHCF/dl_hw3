{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# phon list and map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoneme_list as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stat = pl.N_STATES\n",
    "n_phon = pl.N_PHONEMES\n",
    "p_list = pl.PHONEME_LIST\n",
    "p_map = pl.PHONEME_MAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "46\n",
      "46\n",
      "46\n",
      "['+BREATH+', '+COUGH+', '+NOISE+', '+SMACK+', '+UH+', '+UM+', 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'SIL', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH']\n",
      "['_', '+', '~', '!', '-', '@', 'a', 'A', 'h', 'o', 'w', 'y', 'b', 'c', 'd', 'D', 'e', 'r', 'E', 'f', 'g', 'H', 'i', 'I', 'j', 'k', 'l', 'm', 'n', 'G', 'O', 'Y', 'p', 'R', 's', 'S', '.', 't', 'T', 'u', 'U', 'v', 'W', '?', 'z', 'Z']\n"
     ]
    }
   ],
   "source": [
    "print(n_stat)\n",
    "print(n_phon)\n",
    "print(len(p_list))\n",
    "print(len(p_map))\n",
    "print(p_list)\n",
    "print(p_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get a small piece of train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './../data/wsj0_train.npy'\n",
    "train_label_path = './../data/wsj0_train_merged_labels.npy'\n",
    "\n",
    "val_data_path = './../data/wsj0_dev.npy'\n",
    "val_label_path = './../data/wsj0_dev_merged_labels.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(train_data_path, encoding='bytes')\n",
    "train_label = np.load(train_label_path)\n",
    "\n",
    "val_data = np.load(val_data_path, encoding='bytes')\n",
    "val_label = np.load(val_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample = train_data[:10]\n",
    "train_label_sample = train_label[:10]\n",
    "\n",
    "val_data_sample = val_data[:10]\n",
    "val_label_sample = val_label[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phonome dataset\n",
    "class PhonDataset(Dataset):\n",
    "    def __init__(self, phon_list, labels):\n",
    "        self.data = [torch.tensor(phon) for phon in phon_list]\n",
    "        self.labels = torch.tensor(labels)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        data = self.data[i]\n",
    "        label = self.labels[i]\n",
    "        return data.to(DEVICE), label.to(DEVICE)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# collate_phon return your data sorted by length\n",
    "def collate_phon(phon_list):\n",
    "    inputs, targets = zip(*phon_list)\n",
    "    lens = [len(inputs) for phon in inputs]\n",
    "    seq_order = sorted(range(len(lens)), key=lens.__getitem__, reverse=True)\n",
    "    inputs = [inputs[i] for i in seq_order]\n",
    "    targets = [targets[i] for i in seq_order]\n",
    "    return inputs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phonome dataloader\n",
    "# Model that takes packed sequences in training\n",
    "class PackedPhonModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, nlayers, stop):\n",
    "        super(PackedPhonModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers = nlayers\n",
    "        self.embedding = nn.Embedding(vocab_size , embed_size)\n",
    "        self.rnn = nn.LSTM(input_size = embed_size, hidden_size = hidden_size,\n",
    "                           num_layers = nlayers) # 1 layer, batch_size = False\n",
    "        self.scoring = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    # stop here\n",
    "    def forward(self,phon_list): # list\n",
    "        batch_size = len(phon_list)\n",
    "        lens = [len(phon) for phon in phon_list] # lens of all lines (already sorted)\n",
    "        bounds = [0]\n",
    "        for l in lens:\n",
    "            bounds.append(bounds[-1]+l) # bounds of all lines in the concatenated sequence\n",
    "        seq_concat = torch.cat(seq_list) # concatenated sequence\n",
    "        embed_concat = self.embedding(seq_concat) # concatenated embeddings\n",
    "        embed_list = [embed_concat[bounds[i]:bounds[i+1]] for i in range(batch_size)] # embeddings per line\n",
    "        packed_input = rnn.pack_sequence(embed_list) # packed version\n",
    "        hidden = None\n",
    "        output_packed,hidden = self.rnn(packed_input,hidden)\n",
    "        output_padded, _ = rnn.pad_packed_sequence(output_packed) # unpacked output (padded)\n",
    "        output_flatten = torch.cat([output_padded[:lens[i],i] for i in range(batch_size)]) # concatenated output\n",
    "        scores_flatten = self.scoring(output_flatten) # concatenated logits\n",
    "        return scores_flatten # return concatenated logits\n",
    "    \n",
    "    def generate(self, seq, n_words): # L x V\n",
    "        generated_words = []\n",
    "        embed = self.embedding(seq).unsqueeze(1) # L x 1 x E\n",
    "        hidden = None\n",
    "        output_lstm, hidden = self.rnn(embed,hidden) # L x 1 x H\n",
    "        output = output_lstm[-1] # 1 x H\n",
    "        scores = self.scoring(output) # 1 x V\n",
    "        _,current_word = torch.max(scores,dim=1) # 1 x 1\n",
    "        generated_words.append(current_word)\n",
    "        if n_words > 1:\n",
    "            for i in range(n_words-1):\n",
    "                embed = self.embedding(current_word).unsqueeze(0) # 1 x 1 x E\n",
    "                output_lstm, hidden = self.rnn(embed,hidden) # 1 x 1 x H\n",
    "                output = output_lstm[0] # 1 x H\n",
    "                scores = self.scoring(output) # V\n",
    "                _,current_word = torch.max(scores,dim=1) # 1\n",
    "                generated_words.append(current_word)\n",
    "                if current_word[0].item()==self.stop: # If end of line\n",
    "                    break\n",
    "        return torch.cat(generated_words,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_packed(model, optimizer, train_loader, val_loader):\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\") # sum instead of averaging, to take into account the different lengths\n",
    "    criterion = criterion.to(DEVICE)\n",
    "    batch_id=0\n",
    "    before = time.time()\n",
    "    print(\"Training\", len(train_loader), \"number of batches\")\n",
    "    for inputs,targets in train_loader: # lists, presorted, preloaded on GPU\n",
    "        batch_id+=1\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,torch.cat(targets)) # criterion of the concatenated output\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_id % 100 == 0:\n",
    "            after = time.time()\n",
    "            nwords = np.sum(np.array([len(l) for l in inputs]))\n",
    "            lpw = loss.item() / nwords\n",
    "            print(\"Time elapsed: \", after - before)\n",
    "            print(\"At batch\",batch_id)\n",
    "            print(\"Training loss per word:\",lpw)\n",
    "            print(\"Training perplexity :\",np.exp(lpw))\n",
    "            before = after\n",
    "    \n",
    "    val_loss = 0\n",
    "    batch_id=0\n",
    "    nwords = 0\n",
    "    for inputs,targets in val_loader:\n",
    "        nwords += np.sum(np.array([len(l) for l in inputs]))\n",
    "        batch_id+=1\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,torch.cat(targets))\n",
    "        val_loss+=loss.item()\n",
    "    val_lpw = val_loss / nwords\n",
    "    print(\"\\nValidation loss per word:\",val_lpw)\n",
    "    print(\"Validation perplexity :\",np.exp(val_lpw),\"\\n\")\n",
    "    return val_lpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
